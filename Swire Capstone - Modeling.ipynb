{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6e4160-3cc3-44bd-9673-af6668a86cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Capstone Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65963ebf-49b3-43b3-b22e-9696573d0e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# restart python when needed\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37926f8d-e9da-4d6b-8b28-940c95c04a23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37cc6a0-f59b-4af8-85b5-cf03c20dab1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "\n",
    "# Setup the Configuration\n",
    "conf = pyspark.SparkConf()\n",
    "spark_context = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3820b7-dc8f-4ab8-a6cc-9664353e631f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:677)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:1)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:43)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:45)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:47)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw.&lt;init&gt;(command-4448991716500908:49)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$$iw.&lt;init&gt;(command-4448991716500908:51)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read.&lt;init&gt;(command-4448991716500908:53)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$.&lt;init&gt;(command-4448991716500908:57)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$read$.&lt;clinit&gt;(command-4448991716500908)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $line79bad2525c214d039f6387568901948229.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:656)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:677)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:651)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:1)\n\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:43)\n\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:45)\n\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw$$iw.&lt;init&gt;(command-4448991716500908:47)\n\tat $line79bad2525c214d039f6387568901948229.$read$$iw$$iw.&lt;init&gt;(command-4448991716500908:49)\n\tat $line79bad2525c214d039f6387568901948229.$read$$iw.&lt;init&gt;(command-4448991716500908:51)\n\tat $line79bad2525c214d039f6387568901948229.$read.&lt;init&gt;(command-4448991716500908:53)\n\tat $line79bad2525c214d039f6387568901948229.$read$.&lt;init&gt;(command-4448991716500908:57)\n\tat $line79bad2525c214d039f6387568901948229.$read$.&lt;clinit&gt;(command-4448991716500908)\n\tat $line79bad2525c214d039f6387568901948229.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line79bad2525c214d039f6387568901948229.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line79bad2525c214d039f6387568901948229.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /user/hive/warehouse",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/user/hive/warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9cad14b-1b49-4c1f-9f00-fb08bdbca104",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DATE</th><th>MARKET_KEY</th><th>CALORIC_SEGMENT</th><th>CATEGORY</th><th>UNIT_SALES</th><th>DOLLAR_SALES</th><th>MANUFACTURER</th><th>BRAND</th><th>PACKAGE</th><th>ITEM</th></tr></thead><tbody><tr><td>2021-08-21</td><td>524</td><td>DIET/LIGHT</td><td>SSD</td><td>69.0</td><td>389.74</td><td>SWIRE-CC</td><td>DIET YAWN</td><td>12SMALL 12ONE CUP</td><td>YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12</td></tr><tr><td>2022-05-07</td><td>637</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>30.96</td><td>COCOS</td><td>GORGEOUS ORANGEOUS</td><td>12SMALL 12ONE CUP</td><td>GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12</td></tr><tr><td>2022-10-22</td><td>628</td><td>DIET/LIGHT</td><td>ING ENHANCED WATER</td><td>1.0</td><td>2.25</td><td>JOLLYS</td><td>DIGRESS FLAVORED</td><td>20SMALL MULTI JUG</td><td>DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL </td></tr><tr><td>2022-08-13</td><td>216</td><td>REGULAR</td><td>SSD</td><td>3.0</td><td>7.55</td><td>COCOS</td><td>CHERRY FIZZ</td><td>1L MULTI JUG</td><td>KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL </td></tr><tr><td>2022-01-01</td><td>210</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>25.96</td><td>COCOS</td><td>RADIANT'S</td><td>12SMALL 12ONE CUP</td><td>RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12</td></tr><tr><td>2021-11-27</td><td>278</td><td>REGULAR</td><td>SSD</td><td>112.0</td><td>179.0</td><td>SWIRE-CC</td><td>ROOT BEER WONDER</td><td>2L MULTI JUG</td><td>JUMPIN JACKS GENTLE DRINK ROOT BEER JUG 67.6 LIQUID SMALL </td></tr><tr><td>2022-03-19</td><td>220</td><td>DIET/LIGHT</td><td>SPARKLING WATER</td><td>21.0</td><td>77.61</td><td>JOLLYS</td><td>BUBBLE JOY</td><td>12SMALL 8ONE CUP</td><td>BUBBLE JOY SPARKLING WATER RAZZ  BUBBLE JOY NO CALORIES CUP 12 LIQUID SMALL X8</td></tr><tr><td>2021-11-27</td><td>499</td><td>DIET/LIGHT</td><td>SSD</td><td>3.0</td><td>10.0</td><td>JOLLYS</td><td>HILL MOISTURE ZERO SUGAR</td><td>.5L 6ONE JUG</td><td>RAINING ZERO SUGAR GENTLE DRINK AVOCADO  ZERO CALORIE JUG 16.9 LIQUID SMALL X6</td></tr><tr><td>2021-07-17</td><td>754</td><td>DIET/LIGHT</td><td>SSD</td><td>19.0</td><td>96.67</td><td>JOLLYS</td><td>PAPI ZERO SUGAR CHERRY</td><td>12SMALL 12ONE CUP</td><td>WILD RED  PAPI GENTLE DRINK WILD RED  COLA CUP 12 LIQUID SMALL X12</td></tr><tr><td>2022-04-02</td><td>895</td><td>REGULAR</td><td>SSD</td><td>57.0</td><td>173.33</td><td>JOLLYS</td><td>HILL MOISTURE MAJOR MELON</td><td>.5L 6ONE JUG</td><td>RAINING GENTLE DRINK MAJOR CANES  AVOCADO  JUG 16.9 LIQUID SMALL X6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2021-08-21",
         524,
         "DIET/LIGHT",
         "SSD",
         69.0,
         389.74,
         "SWIRE-CC",
         "DIET YAWN",
         "12SMALL 12ONE CUP",
         "YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12"
        ],
        [
         "2022-05-07",
         637,
         "REGULAR",
         "SSD",
         4.0,
         30.96,
         "COCOS",
         "GORGEOUS ORANGEOUS",
         "12SMALL 12ONE CUP",
         "GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12"
        ],
        [
         "2022-10-22",
         628,
         "DIET/LIGHT",
         "ING ENHANCED WATER",
         1.0,
         2.25,
         "JOLLYS",
         "DIGRESS FLAVORED",
         "20SMALL MULTI JUG",
         "DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL "
        ],
        [
         "2022-08-13",
         216,
         "REGULAR",
         "SSD",
         3.0,
         7.55,
         "COCOS",
         "CHERRY FIZZ",
         "1L MULTI JUG",
         "KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL "
        ],
        [
         "2022-01-01",
         210,
         "REGULAR",
         "SSD",
         4.0,
         25.96,
         "COCOS",
         "RADIANT'S",
         "12SMALL 12ONE CUP",
         "RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12"
        ],
        [
         "2021-11-27",
         278,
         "REGULAR",
         "SSD",
         112.0,
         179.0,
         "SWIRE-CC",
         "ROOT BEER WONDER",
         "2L MULTI JUG",
         "JUMPIN JACKS GENTLE DRINK ROOT BEER JUG 67.6 LIQUID SMALL "
        ],
        [
         "2022-03-19",
         220,
         "DIET/LIGHT",
         "SPARKLING WATER",
         21.0,
         77.61,
         "JOLLYS",
         "BUBBLE JOY",
         "12SMALL 8ONE CUP",
         "BUBBLE JOY SPARKLING WATER RAZZ  BUBBLE JOY NO CALORIES CUP 12 LIQUID SMALL X8"
        ],
        [
         "2021-11-27",
         499,
         "DIET/LIGHT",
         "SSD",
         3.0,
         10.0,
         "JOLLYS",
         "HILL MOISTURE ZERO SUGAR",
         ".5L 6ONE JUG",
         "RAINING ZERO SUGAR GENTLE DRINK AVOCADO  ZERO CALORIE JUG 16.9 LIQUID SMALL X6"
        ],
        [
         "2021-07-17",
         754,
         "DIET/LIGHT",
         "SSD",
         19.0,
         96.67,
         "JOLLYS",
         "PAPI ZERO SUGAR CHERRY",
         "12SMALL 12ONE CUP",
         "WILD RED  PAPI GENTLE DRINK WILD RED  COLA CUP 12 LIQUID SMALL X12"
        ],
        [
         "2022-04-02",
         895,
         "REGULAR",
         "SSD",
         57.0,
         173.33,
         "JOLLYS",
         "HILL MOISTURE MAJOR MELON",
         ".5L 6ONE JUG",
         "RAINING GENTLE DRINK MAJOR CANES  AVOCADO  JUG 16.9 LIQUID SMALL X6"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MARKET_KEY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CALORIC_SEGMENT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CATEGORY",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UNIT_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DOLLAR_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "MANUFACTURER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "BRAND",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PACKAGE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ITEM",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read parquet file using read.parquet()\n",
    "parquetDF = spark.read.parquet(\"/FileStore/tables/capstone/FACT_MARKET_DEMAND.parquet\")\n",
    "\n",
    "# Show the DataFrame\n",
    "display(parquetDF.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79de3baa-c7b1-4cd8-8e59-9902f00c2360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[84]: [('DATE', 'string'),\n ('MARKET_KEY', 'bigint'),\n ('CALORIC_SEGMENT', 'string'),\n ('CATEGORY', 'string'),\n ('UNIT_SALES', 'double'),\n ('DOLLAR_SALES', 'double'),\n ('MANUFACTURER', 'string'),\n ('BRAND', 'string'),\n ('PACKAGE', 'string'),\n ('ITEM', 'string')]"
     ]
    }
   ],
   "source": [
    "parquetDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940567d9-74ed-439c-875f-7989068dfe8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ZIP_CODE</th><th>MARKET_KEY</th></tr></thead><tbody><tr><td>57714</td><td>161</td></tr><tr><td>57772</td><td>161</td></tr><tr><td>67701</td><td>1135</td></tr><tr><td>67740</td><td>1135</td></tr><tr><td>67748</td><td>1135</td></tr><tr><td>69334</td><td>352</td></tr><tr><td>69336</td><td>352</td></tr><tr><td>69341</td><td>352</td></tr><tr><td>69347</td><td>161</td></tr><tr><td>69358</td><td>352</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "57714",
         "161"
        ],
        [
         "57772",
         "161"
        ],
        [
         "67701",
         "1135"
        ],
        [
         "67740",
         "1135"
        ],
        [
         "67748",
         "1135"
        ],
        [
         "69334",
         "352"
        ],
        [
         "69336",
         "352"
        ],
        [
         "69341",
         "352"
        ],
        [
         "69347",
         "161"
        ],
        [
         "69358",
         "352"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ZIP_CODE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MARKET_KEY",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading in the zipmap csv file\n",
    "zipmap = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/capstone/zip_to_market_unit_mapping.csv\")\n",
    "\n",
    "# Look at the table\n",
    "display(zipmap.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe01bac-e49e-4ea9-a38c-f52b5f60218b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DATE: date (nullable = true)\n |-- MARKET_KEY: short (nullable = true)\n |-- CALORIC_SEGMENT: string (nullable = true)\n |-- CATEGORY: string (nullable = true)\n |-- UNIT_SALES: double (nullable = true)\n |-- DOLLAR_SALES: double (nullable = true)\n |-- MANUFACTURER: string (nullable = true)\n |-- BRAND: string (nullable = true)\n |-- PACKAGE: string (nullable = true)\n |-- ITEM: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date # rewrite this to use the already loaded package F\n",
    "\n",
    "# convert MARKET_KEY to shorttype\n",
    "parquetDF = parquetDF.withColumn(\"MARKET_KEY\", F.col(\"MARKET_KEY\").cast(ShortType()))\n",
    "\n",
    "# Convert the 'DATE' column to datetime\n",
    "parquetDF = parquetDF.withColumn('DATE', F.to_date(parquetDF['DATE'], 'yyyy-MM-dd'))\n",
    "\n",
    "# Check the DataFrame schema again to confirm the change\n",
    "parquetDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e01011-13a2-488e-9978-a8e3f776f25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DATE</th><th>MARKET_KEY</th><th>CALORIC_SEGMENT</th><th>CATEGORY</th><th>UNIT_SALES</th><th>DOLLAR_SALES</th><th>MANUFACTURER</th><th>BRAND</th><th>PACKAGE</th><th>ITEM</th></tr></thead><tbody><tr><td>2021-08-21</td><td>524</td><td>DIET/LIGHT</td><td>SSD</td><td>69.0</td><td>389.74</td><td>SWIRE-CC</td><td>DIET YAWN</td><td>12SMALL 12ONE CUP</td><td>YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12</td></tr><tr><td>2022-05-07</td><td>637</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>30.96</td><td>COCOS</td><td>GORGEOUS ORANGEOUS</td><td>12SMALL 12ONE CUP</td><td>GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12</td></tr><tr><td>2022-10-22</td><td>628</td><td>DIET/LIGHT</td><td>ING ENHANCED WATER</td><td>1.0</td><td>2.25</td><td>JOLLYS</td><td>DIGRESS FLAVORED</td><td>20SMALL MULTI JUG</td><td>DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL </td></tr><tr><td>2022-08-13</td><td>216</td><td>REGULAR</td><td>SSD</td><td>3.0</td><td>7.55</td><td>COCOS</td><td>CHERRY FIZZ</td><td>1L MULTI JUG</td><td>KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL </td></tr><tr><td>2022-01-01</td><td>210</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>25.96</td><td>COCOS</td><td>RADIANT'S</td><td>12SMALL 12ONE CUP</td><td>RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2021-08-21",
         524,
         "DIET/LIGHT",
         "SSD",
         69.0,
         389.74,
         "SWIRE-CC",
         "DIET YAWN",
         "12SMALL 12ONE CUP",
         "YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12"
        ],
        [
         "2022-05-07",
         637,
         "REGULAR",
         "SSD",
         4.0,
         30.96,
         "COCOS",
         "GORGEOUS ORANGEOUS",
         "12SMALL 12ONE CUP",
         "GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12"
        ],
        [
         "2022-10-22",
         628,
         "DIET/LIGHT",
         "ING ENHANCED WATER",
         1.0,
         2.25,
         "JOLLYS",
         "DIGRESS FLAVORED",
         "20SMALL MULTI JUG",
         "DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL "
        ],
        [
         "2022-08-13",
         216,
         "REGULAR",
         "SSD",
         3.0,
         7.55,
         "COCOS",
         "CHERRY FIZZ",
         "1L MULTI JUG",
         "KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL "
        ],
        [
         "2022-01-01",
         210,
         "REGULAR",
         "SSD",
         4.0,
         25.96,
         "COCOS",
         "RADIANT'S",
         "12SMALL 12ONE CUP",
         "RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "MARKET_KEY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CALORIC_SEGMENT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CATEGORY",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UNIT_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DOLLAR_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "MANUFACTURER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "BRAND",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PACKAGE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ITEM",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parquetDF.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249c978d-5f3a-4e95-aad6-ba5b6f270656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ZIP_CODE: string (nullable = true)\n |-- MARKET_KEY: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "zipmap.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eec678d-0144-4b4f-a2ef-1da954836500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Market Key:  1\nMaximum Market Key:  6802\n"
     ]
    }
   ],
   "source": [
    "min_key = parquetDF.agg(F.min('MARKET_KEY')).collect()[0][0]\n",
    "max_key = parquetDF.agg(F.max('MARKET_KEY')).collect()[0][0]\n",
    "\n",
    "print(\"Minimum Market Key: \", min_key)\n",
    "print(\"Maximum Market Key: \", max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d325b36a-39fe-4726-92cf-ec76940f3e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col # should be included from previous load\n",
    "\n",
    "# Convert the 'MARKET_KEY' column in 'zipmap' to ShortType\n",
    "zipmap = zipmap.withColumn(\"MARKET_KEY\", F.col(\"MARKET_KEY\").cast(ShortType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26fee74-086b-47e0-bf2d-85eeeb6d56e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'MARKET_KEY' and aggregate 'ZIP_CODE' into a list\n",
    "zipmap_agg = zipmap.groupBy('MARKET_KEY').agg(F.collect_list('ZIP_CODE').alias('ZIP_CODES'))\n",
    "\n",
    "# Join the dataframes\n",
    "mergedDF = parquetDF.join(zipmap_agg, on='MARKET_KEY', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab08131-a92d-4510-95c4-4563d8088aa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>MARKET_KEY</th><th>DATE</th><th>CALORIC_SEGMENT</th><th>CATEGORY</th><th>UNIT_SALES</th><th>DOLLAR_SALES</th><th>MANUFACTURER</th><th>BRAND</th><th>PACKAGE</th><th>ITEM</th><th>ZIP_CODES</th></tr></thead><tbody><tr><td>524</td><td>2021-08-21</td><td>DIET/LIGHT</td><td>SSD</td><td>69.0</td><td>389.74</td><td>SWIRE-CC</td><td>DIET YAWN</td><td>12SMALL 12ONE CUP</td><td>YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12</td><td>List(85087, 85266, 85324, 85331, 85377, 86333)</td></tr><tr><td>637</td><td>2022-05-07</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>30.96</td><td>COCOS</td><td>GORGEOUS ORANGEOUS</td><td>12SMALL 12ONE CUP</td><td>GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12</td><td>List(98133, 98155, 98028, 98103, 98115, 98117, 98125)</td></tr><tr><td>628</td><td>2022-10-22</td><td>DIET/LIGHT</td><td>ING ENHANCED WATER</td><td>1.0</td><td>2.25</td><td>JOLLYS</td><td>DIGRESS FLAVORED</td><td>20SMALL MULTI JUG</td><td>DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL </td><td>List(95838)</td></tr><tr><td>216</td><td>2022-08-13</td><td>REGULAR</td><td>SSD</td><td>3.0</td><td>7.55</td><td>COCOS</td><td>CHERRY FIZZ</td><td>1L MULTI JUG</td><td>KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL </td><td>List(86047, 85924, 85928, 85933, 85936, 85937, 85939, 86025, 86028, 86029, 86031, 86032, 86034, 86042)</td></tr><tr><td>210</td><td>2022-01-01</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>25.96</td><td>COCOS</td><td>RADIANT'S</td><td>12SMALL 12ONE CUP</td><td>RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12</td><td>List(85325, 85344, 85348, 85357, 85360, 86403, 86404, 86405, 86406, 86436, 86438)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         524,
         "2021-08-21",
         "DIET/LIGHT",
         "SSD",
         69.0,
         389.74,
         "SWIRE-CC",
         "DIET YAWN",
         "12SMALL 12ONE CUP",
         "YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12",
         [
          "85087",
          "85266",
          "85324",
          "85331",
          "85377",
          "86333"
         ]
        ],
        [
         637,
         "2022-05-07",
         "REGULAR",
         "SSD",
         4.0,
         30.96,
         "COCOS",
         "GORGEOUS ORANGEOUS",
         "12SMALL 12ONE CUP",
         "GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12",
         [
          "98133",
          "98155",
          "98028",
          "98103",
          "98115",
          "98117",
          "98125"
         ]
        ],
        [
         628,
         "2022-10-22",
         "DIET/LIGHT",
         "ING ENHANCED WATER",
         1.0,
         2.25,
         "JOLLYS",
         "DIGRESS FLAVORED",
         "20SMALL MULTI JUG",
         "DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL ",
         [
          "95838"
         ]
        ],
        [
         216,
         "2022-08-13",
         "REGULAR",
         "SSD",
         3.0,
         7.55,
         "COCOS",
         "CHERRY FIZZ",
         "1L MULTI JUG",
         "KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL ",
         [
          "86047",
          "85924",
          "85928",
          "85933",
          "85936",
          "85937",
          "85939",
          "86025",
          "86028",
          "86029",
          "86031",
          "86032",
          "86034",
          "86042"
         ]
        ],
        [
         210,
         "2022-01-01",
         "REGULAR",
         "SSD",
         4.0,
         25.96,
         "COCOS",
         "RADIANT'S",
         "12SMALL 12ONE CUP",
         "RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12",
         [
          "85325",
          "85344",
          "85348",
          "85357",
          "85360",
          "86403",
          "86404",
          "86405",
          "86406",
          "86436",
          "86438"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "MARKET_KEY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "CALORIC_SEGMENT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CATEGORY",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UNIT_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DOLLAR_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "MANUFACTURER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "BRAND",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PACKAGE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ITEM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ZIP_CODES",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mergedDF.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dba9da-7b4e-420f-b0fa-aab1a2cce3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 24461424\nNumber of columns: 11\n"
     ]
    }
   ],
   "source": [
    "num_rows = mergedDF.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "num_cols = len(mergedDF.columns)\n",
    "print(f\"Number of columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22527f6b-46bc-421e-b7b0-77920308d14f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in original df: 24461424\nNumber of columns in original df: 10\n"
     ]
    }
   ],
   "source": [
    "num_rows = parquetDF.count()\n",
    "print(f\"Number of rows in original df: {num_rows}\")\n",
    "\n",
    "num_cols = len(parquetDF.columns)\n",
    "print(f\"Number of columns in original df: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c171ef-9084-4008-a200-8e840363cb8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- MARKET_KEY: short (nullable = true)\n |-- DATE: date (nullable = true)\n |-- CALORIC_SEGMENT: string (nullable = true)\n |-- CATEGORY: string (nullable = true)\n |-- UNIT_SALES: double (nullable = true)\n |-- DOLLAR_SALES: double (nullable = true)\n |-- MANUFACTURER: string (nullable = true)\n |-- BRAND: string (nullable = true)\n |-- PACKAGE: string (nullable = true)\n |-- ITEM: string (nullable = true)\n |-- ZIP_CODES: array (nullable = true)\n |    |-- element: string (containsNull = false)\n\n"
     ]
    }
   ],
   "source": [
    "mergedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c82ff7-fa5a-4554-839f-bc63171ac867",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date:  2020-12-05\nMaximum Date:  2023-10-28\n"
     ]
    }
   ],
   "source": [
    "min_date = mergedDF.agg(F.min('DATE')).collect()[0][0]\n",
    "max_date = mergedDF.agg(F.max('DATE')).collect()[0][0]\n",
    "\n",
    "print(\"Minimum Date: \", min_date)\n",
    "print(\"Maximum Date: \", max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0f78cb-e8fe-4f7f-b5a8-4358d61026c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>UNIT_SALES</th><th>DOLLAR_SALES</th></tr></thead><tbody><tr><td>count</td><td>2.4461424E7</td><td>2.4461424E7</td></tr><tr><td>mean</td><td>174.37</td><td>591.14</td></tr><tr><td>stddev</td><td>857.81</td><td>3040.54</td></tr><tr><td>min</td><td>0.04</td><td>0.01</td></tr><tr><td>max</td><td>96776.0</td><td>492591.07</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         2.4461424E7,
         2.4461424E7
        ],
        [
         "mean",
         174.37,
         591.14
        ],
        [
         "stddev",
         857.81,
         3040.54
        ],
        [
         "min",
         0.04,
         0.01
        ],
        [
         "max",
         96776.0,
         492591.07
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UNIT_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DOLLAR_SALES",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = ['UNIT_SALES', 'DOLLAR_SALES']\n",
    "\n",
    "# Get summary statistics for numeric columns\n",
    "summary = mergedDF.select(numeric_cols).describe()\n",
    "\n",
    "# Round the results to 2 decimal places\n",
    "for col in numeric_cols:\n",
    "    summary = summary.withColumn(col, F.round(summary[col], 2))\n",
    "\n",
    "# Show the summary\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc5b4ac-d5dc-4454-8913-7766279ad1d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "mean_unit_sales = mergedDF.select(mean('UNIT_SALES')).collect()[0][0]\n",
    "stddev_unit_sales = mergedDF.select(stddev('UNIT_SALES')).collect()[0][0]\n",
    "\n",
    "mean_dollar_sales = mergedDF.select(mean('DOLLAR_SALES')).collect()[0][0]\n",
    "stddev_dollar_sales = mergedDF.select(stddev('DOLLAR_SALES')).collect()[0][0]\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "mergedDF = mergedDF.withColumn('zscore_UNIT_SALES', (col('UNIT_SALES') - mean_unit_sales) / stddev_unit_sales)\n",
    "mergedDF = mergedDF.withColumn('zscore_DOLLAR_SALES', (col('DOLLAR_SALES') - mean_dollar_sales) / stddev_dollar_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587e914d-e483-4f0e-8d1e-9bec53f17b58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>MARKET_KEY</th><th>DATE</th><th>CALORIC_SEGMENT</th><th>CATEGORY</th><th>UNIT_SALES</th><th>DOLLAR_SALES</th><th>MANUFACTURER</th><th>BRAND</th><th>PACKAGE</th><th>ITEM</th><th>ZIP_CODES</th><th>zscore_UNIT_SALES</th><th>zscore_DOLLAR_SALES</th></tr></thead><tbody><tr><td>524</td><td>2021-08-21</td><td>DIET/LIGHT</td><td>SSD</td><td>69.0</td><td>389.74</td><td>SWIRE-CC</td><td>DIET YAWN</td><td>12SMALL 12ONE CUP</td><td>YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12</td><td>List(85087, 85266, 85324, 85331, 85377, 86333)</td><td>-0.12283715225737275</td><td>-0.06623827270113472</td></tr><tr><td>637</td><td>2022-05-07</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>30.96</td><td>COCOS</td><td>GORGEOUS ORANGEOUS</td><td>12SMALL 12ONE CUP</td><td>GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12</td><td>List(98133, 98155, 98028, 98103, 98115, 98117, 98125)</td><td>-0.198611473644821</td><td>-0.1842370597245729</td></tr><tr><td>628</td><td>2022-10-22</td><td>DIET/LIGHT</td><td>ING ENHANCED WATER</td><td>1.0</td><td>2.25</td><td>JOLLYS</td><td>DIGRESS FLAVORED</td><td>20SMALL MULTI JUG</td><td>DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL </td><td>List(95838)</td><td>-0.2021087500165494</td><td>-0.19367946224267013</td></tr><tr><td>216</td><td>2022-08-13</td><td>REGULAR</td><td>SSD</td><td>3.0</td><td>7.55</td><td>COCOS</td><td>CHERRY FIZZ</td><td>1L MULTI JUG</td><td>KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL </td><td>List(86047, 85924, 85928, 85933, 85936, 85937, 85939, 86025, 86028, 86029, 86031, 86032, 86034, 86042)</td><td>-0.19977723243539713</td><td>-0.19193635066670653</td></tr><tr><td>210</td><td>2022-01-01</td><td>REGULAR</td><td>SSD</td><td>4.0</td><td>25.96</td><td>COCOS</td><td>RADIANT'S</td><td>12SMALL 12ONE CUP</td><td>RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12</td><td>List(85325, 85344, 85348, 85357, 85360, 86403, 86404, 86405, 86406, 86436, 86438)</td><td>-0.198611473644821</td><td>-0.18588150460755745</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         524,
         "2021-08-21",
         "DIET/LIGHT",
         "SSD",
         69.0,
         389.74,
         "SWIRE-CC",
         "DIET YAWN",
         "12SMALL 12ONE CUP",
         "YAWN ZERO SUGAR GENTLE DRINK SUPER-JUICE  DURIAN  CUP 12 LIQUID SMALL X12",
         [
          "85087",
          "85266",
          "85324",
          "85331",
          "85377",
          "86333"
         ],
         -0.12283715225737275,
         -0.06623827270113472
        ],
        [
         637,
         "2022-05-07",
         "REGULAR",
         "SSD",
         4.0,
         30.96,
         "COCOS",
         "GORGEOUS ORANGEOUS",
         "12SMALL 12ONE CUP",
         "GORGEOUS SUNSET OUS GENTLE DRINK AVOCADO  CUP 12 LIQUID SMALL X12",
         [
          "98133",
          "98155",
          "98028",
          "98103",
          "98115",
          "98117",
          "98125"
         ],
         -0.198611473644821,
         -0.1842370597245729
        ],
        [
         628,
         "2022-10-22",
         "DIET/LIGHT",
         "ING ENHANCED WATER",
         1.0,
         2.25,
         "JOLLYS",
         "DIGRESS FLAVORED",
         "20SMALL MULTI JUG",
         "DIGRESS ZERO NUTRIENT ENHANCED WATER BVRG PURPLE  ZERO CALORIE JUG 20 LIQUID SMALL ",
         [
          "95838"
         ],
         -0.2021087500165494,
         -0.19367946224267013
        ],
        [
         216,
         "2022-08-13",
         "REGULAR",
         "SSD",
         3.0,
         7.55,
         "COCOS",
         "CHERRY FIZZ",
         "1L MULTI JUG",
         "KOOL! RED  GENTLE DRINK RED  COLA CONTOUR JUG 33.8 LIQUID SMALL ",
         [
          "86047",
          "85924",
          "85928",
          "85933",
          "85936",
          "85937",
          "85939",
          "86025",
          "86028",
          "86029",
          "86031",
          "86032",
          "86034",
          "86042"
         ],
         -0.19977723243539713,
         -0.19193635066670653
        ],
        [
         210,
         "2022-01-01",
         "REGULAR",
         "SSD",
         4.0,
         25.96,
         "COCOS",
         "RADIANT'S",
         "12SMALL 12ONE CUP",
         "RADIANT'S GENTLE DRINK GINGER ALE CUP 12 LIQUID SMALL X12",
         [
          "85325",
          "85344",
          "85348",
          "85357",
          "85360",
          "86403",
          "86404",
          "86405",
          "86406",
          "86436",
          "86438"
         ],
         -0.198611473644821,
         -0.18588150460755745
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "MARKET_KEY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "CALORIC_SEGMENT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CATEGORY",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UNIT_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DOLLAR_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "MANUFACTURER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "BRAND",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PACKAGE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ITEM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ZIP_CODES",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "zscore_UNIT_SALES",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "zscore_DOLLAR_SALES",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mergedDF.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53e8d9f-04fe-4080-bd89-e2295f54559e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Null Count</th></tr></thead><tbody><tr><td>DATE</td><td>0</td></tr><tr><td>MARKET_KEY</td><td>0</td></tr><tr><td>CALORIC_SEGMENT</td><td>59725</td></tr><tr><td>CATEGORY</td><td>0</td></tr><tr><td>UNIT_SALES</td><td>0</td></tr><tr><td>DOLLAR_SALES</td><td>0</td></tr><tr><td>MANUFACTURER</td><td>0</td></tr><tr><td>BRAND</td><td>0</td></tr><tr><td>PACKAGE</td><td>0</td></tr><tr><td>ITEM</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DATE",
         0
        ],
        [
         "MARKET_KEY",
         0
        ],
        [
         "CALORIC_SEGMENT",
         59725
        ],
        [
         "CATEGORY",
         0
        ],
        [
         "UNIT_SALES",
         0
        ],
        [
         "DOLLAR_SALES",
         0
        ],
        [
         "MANUFACTURER",
         0
        ],
        [
         "BRAND",
         0
        ],
        [
         "PACKAGE",
         0
        ],
        [
         "ITEM",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Null Count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate null counts\n",
    "null_counts = mergedDF.agg(*[F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in parquetDF.columns])\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "null_counts_pd = null_counts.toPandas()\n",
    "\n",
    "# Transpose the DataFrame\n",
    "null_counts_pd = null_counts_pd.transpose()\n",
    "\n",
    "# Reset the index\n",
    "null_counts_pd.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "null_counts_pd.columns = ['Column', 'Null Count']\n",
    "\n",
    "# Display the DataFrame\n",
    "display(null_counts_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da8f8ba-9b70-4d5f-930c-1f385a784361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- MARKET_KEY: short (nullable = true)\n |-- DATE: date (nullable = true)\n |-- CALORIC_SEGMENT: string (nullable = true)\n |-- CATEGORY: string (nullable = true)\n |-- UNIT_SALES: double (nullable = true)\n |-- DOLLAR_SALES: double (nullable = true)\n |-- MANUFACTURER: string (nullable = true)\n |-- BRAND: string (nullable = true)\n |-- PACKAGE: string (nullable = true)\n |-- ITEM: string (nullable = true)\n |-- ZIP_CODES: array (nullable = true)\n |    |-- element: string (containsNull = false)\n |-- zscore_UNIT_SALES: double (nullable = true)\n |-- zscore_DOLLAR_SALES: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "mergedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03be67c6-9520-49bf-ab42-9bb513c96d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, ShortType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Downcast numeric columns\n",
    "mergedDF = mergedDF.withColumn('MARKET_KEY', mergedDF['MARKET_KEY'].cast(IntegerType()))\n",
    "mergedDF = mergedDF.withColumn('UNIT_SALES', F.round(mergedDF['UNIT_SALES'].cast(FloatType()), 2))\n",
    "mergedDF = mergedDF.withColumn('DOLLAR_SALES', F.round(mergedDF['DOLLAR_SALES'].cast(FloatType()), 2))\n",
    "mergedDF = mergedDF.withColumn('zscore_UNIT_SALES', F.round(mergedDF['zscore_UNIT_SALES'].cast(FloatType()), 2))\n",
    "mergedDF = mergedDF.withColumn('zscore_DOLLAR_SALES', F.round(mergedDF['zscore_DOLLAR_SALES'].cast(FloatType()), 2))\n",
    "\n",
    "# Extract year, month, and day from DATE and cast to ShortType\n",
    "mergedDF = mergedDF.withColumn('Year', F.year('DATE').cast(ShortType()))\n",
    "mergedDF = mergedDF.withColumn('Month', F.month('DATE').cast(ShortType()))\n",
    "mergedDF = mergedDF.withColumn('Day', F.dayofmonth('DATE').cast(ShortType()))\n",
    "\n",
    "# # Convert object columns to category, drop original columns, and rename new ones\n",
    "# indexer_cols = ['CALORIC_SEGMENT', 'CATEGORY', 'MANUFACTURER', 'BRAND', 'PACKAGE', 'ITEM']\n",
    "# for col in indexer_cols:\n",
    "#     indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n",
    "#     mergedDF = indexer.fit(mergedDF).transform(mergedDF)\n",
    "#     mergedDF = mergedDF.drop(col)\n",
    "#     mergedDF = mergedDF.withColumnRenamed(col+\"_index\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4e2e85-0bad-4ee2-9b73-c10a917a3b7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- MARKET_KEY: integer (nullable = true)\n |-- DATE: date (nullable = true)\n |-- CALORIC_SEGMENT: string (nullable = true)\n |-- CATEGORY: string (nullable = true)\n |-- UNIT_SALES: float (nullable = true)\n |-- DOLLAR_SALES: float (nullable = true)\n |-- MANUFACTURER: string (nullable = true)\n |-- BRAND: string (nullable = true)\n |-- PACKAGE: string (nullable = true)\n |-- ITEM: string (nullable = true)\n |-- ZIP_CODES: array (nullable = true)\n |    |-- element: string (containsNull = false)\n |-- zscore_UNIT_SALES: float (nullable = true)\n |-- zscore_DOLLAR_SALES: float (nullable = true)\n |-- Year: short (nullable = true)\n |-- Month: short (nullable = true)\n |-- Day: short (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "mergedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e54b9e2-1a5e-4ddf-9101-1b0b86eceba3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669076e4-580a-44a4-848b-9d7c4013b811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6e620c-64b9-4b7d-a70f-91f7f4fa77ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.installPyPI(\"xgboost\")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c62eed5-bccb-4461-93ca-33972fd95649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergedDF = mergedDF.drop('DATE', 'ZIP_CODES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b53c37f-e0bc-45fc-9308-64e8a78fa18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3425296378987475>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorAssembler\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m XGBoostClassifier\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Define the input columns for the VectorAssembler\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m input_cols \u001B[38;5;241m=\u001B[39m [col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m mergedDF\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m: cannot import name 'XGBoostClassifier' from 'pyspark.ml.classification' (/databricks/spark/python/pyspark/ml/classification.py)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\nFile \u001B[0;32m<command-3425296378987475>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorAssembler\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m XGBoostClassifier\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Define the input columns for the VectorAssembler\u001B[39;00m\n\u001B[1;32m      5\u001B[0m input_cols \u001B[38;5;241m=\u001B[39m [col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m mergedDF\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\n\u001B[0;31mImportError\u001B[0m: cannot import name 'XGBoostClassifier' from 'pyspark.ml.classification' (/databricks/spark/python/pyspark/ml/classification.py)",
       "errorSummary": "<span class='ansi-red-fg'>ImportError</span>: cannot import name 'XGBoostClassifier' from 'pyspark.ml.classification' (/databricks/spark/python/pyspark/ml/classification.py)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import XGBoostClassifier\n",
    "\n",
    "# Define the input columns for the VectorAssembler\n",
    "input_cols = [col for col in mergedDF.columns if col != 'target']\n",
    "\n",
    "# Initialize the VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "\n",
    "# Transform the DataFrame\n",
    "mergedDF = assembler.transform(mergedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f0b32f-fb4f-4471-ad51-1c25f7d5fd09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3425296378987474>:5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m train_data, test_data \u001B[38;5;241m=\u001B[39m df_transformed\u001B[38;5;241m.\u001B[39mrandomSplit([\u001B[38;5;241m0.7\u001B[39m, \u001B[38;5;241m0.3\u001B[39m])\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Check the number of rows in each DataFrame\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of training records: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_data\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of testing records : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_data\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1214\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n",
       "\u001B[1;32m   1192\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   1193\u001B[0m \n",
       "\u001B[1;32m   1194\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1212\u001B[0m \u001B[38;5;124;03m    3\u001B[39;00m\n",
       "\u001B[1;32m   1213\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1214\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o4092.count.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 228.0 failed 1 times, most recent failure: Lost task 1.0 in stage 228.0 (TID 295) (ip-10-172-166-117.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$9567/1328659330: (string) => double).\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:246)\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
       "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:397)\n",
       "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:392)\n",
       "\t... 29 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3401)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3332)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3321)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3321)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1438)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1438)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1438)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3614)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3552)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3540)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$9567/1328659330: (string) => double).\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:246)\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
       "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:397)\n",
       "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:392)\n",
       "\t... 29 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-3425296378987474>:5\u001B[0m\n\u001B[1;32m      2\u001B[0m train_data, test_data \u001B[38;5;241m=\u001B[39m df_transformed\u001B[38;5;241m.\u001B[39mrandomSplit([\u001B[38;5;241m0.7\u001B[39m, \u001B[38;5;241m0.3\u001B[39m])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Check the number of rows in each DataFrame\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of training records: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_data\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of testing records : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_data\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1214\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   1193\u001B[0m \n\u001B[1;32m   1194\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1212\u001B[0m \u001B[38;5;124;03m    3\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1214\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o4092.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 228.0 failed 1 times, most recent failure: Lost task 1.0 in stage 228.0 (TID 295) (ip-10-172-166-117.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$9567/1328659330: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:246)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:397)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:392)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3401)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3332)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3321)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3321)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1438)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3614)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3552)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3540)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$9567/1328659330: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:246)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:397)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:392)\n\t... 29 more\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 228.0 failed 1 times, most recent failure: Lost task 1.0 in stage 228.0 (TID 295) (ip-10-172-166-117.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$9567/1328659330: (string) => double).",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df_transformed.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Check the number of rows in each DataFrame\n",
    "print(f\"Number of training records: {train_data.count()}\")\n",
    "print(f\"Number of testing records : {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a149b2-ef6e-4326-9c91-f8dd4c6e351e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_estimators\": 1000,\n",
    "    \"num_boost_round\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38b1f03-a5de-4ee4-a67e-3b3a1ef853b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc6bb5e-da70-488b-b318-32d02d39ea5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47a7cb3a-f4bf-4669-8728-ad516a6c8d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c424971e-a2c0-48b4-acb4-c4eca9a16b7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the window based on the date column\n",
    "window = Window.orderBy(mergedDF['DATE']).rowsBetween(-11, 0)\n",
    "\n",
    "# Calculate the rolling mean and standard deviation\n",
    "rolmean = mergedDF.withColumn('rolling_mean', F.mean('UNIT_SALES').over(window))\n",
    "rolstd = mergedDF.withColumn('rolling_std', F.stddev('UNIT_SALES').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ca5b67-9a35-4fd5-8602-871a9148adf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the rolling mean and standard deviation\n",
    "df = mergedDF.withColumn('rolling_mean', F.mean('UNIT_SALES').over(window))\n",
    "df = df.withColumn('rolling_std', F.stddev('UNIT_SALES').over(window))\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af06f804-0db6-498e-80db-3d8fa408511c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data into two parts\n",
    "X = mergedDF.select('UNIT_SALES').toPandas().values\n",
    "split = len(X) // 2\n",
    "X1, X2 = X[0:split], X[split:]\n",
    "\n",
    "# Compare the mean and variance of each group\n",
    "mean1, mean2 = X1.mean(), X2.mean()\n",
    "var1, var2 = X1.var(), X2.var()\n",
    "\n",
    "print('mean1=%f, mean2=%f' % (mean1, mean2))\n",
    "print('variance1=%f, variance2=%f' % (var1, var2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5fd6292-83a4-447b-9de6-e30b8f7cc28e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine the number of partitions\n",
    "num_partitions = 5  # Adjust this value based on your data size and available resources\n",
    "\n",
    "# Repartition the DataFrame\n",
    "repartitionedDF = mergedDF.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862fd94c-cd15-4227-bd2f-a9e49cb3af03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_partition(iterator):\n",
    "    for row in iterator:\n",
    "        # Process each row in the partition here\n",
    "        pass\n",
    "\n",
    "repartitionedDF.rdd.foreachPartition(process_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ac5d3ef-caf4-41af-a0ef-cdf24790926b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "import random\n",
    "\n",
    "# Generate a random index\n",
    "random_index = random.randint(0, num_partitions - 1)\n",
    "\n",
    "# Grab a random partition\n",
    "random_partition = (repartitionedDF\n",
    "                    .rdd\n",
    "                    .mapPartitionsWithIndex(lambda index, iterator: iterator if index == random_index else iter([]))\n",
    "                    .sample(False, 0.1)  # Take a 10% sample of the data\n",
    "                    .collect())\n",
    "\n",
    "# Convert the random partition to a Pandas DataFrame\n",
    "random_partition_df = pd.DataFrame(random_partition, columns=repartitionedDF.columns)\n",
    "\n",
    "# Perform Dickey-Fuller test:\n",
    "print('Results of Dickey-Fuller Test:')\n",
    "dftest = adfuller(random_partition_df['UNIT_SALES'], autolag='AIC')\n",
    "\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5795eae-50aa-4b32-a3c8-4dbbc39c2ed4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(random_partition_df['UNIT_SALES'], lags=50)\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(random_partition_df['UNIT_SALES'], lags=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78dd73a-ef8c-4ac7-a6c4-8cf9d6cc3f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit an ARIMA model\n",
    "model = ARIMA(random_partition_df['UNIT_SALES'], order=(0, 0, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a22e9e-ca21-4f03-81ba-9babdb91a1b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4448991716500908,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Swire Capstone - Modeling",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
